- Title: From Curiosity to Competence: How World Models Interact with the Dynamics of Exploration
- Authors: Fryderyk Mantiuk, Hanqi Zhou, Charley M. Wu
- Publication Date: July 10, 2025
- Key Contributions:
  - The study bridges cognitive theories of intrinsic motivation with reinforcement learning.
  - It investigates how evolving internal representations mediate the trade-off between curiosity and competence.
  - It compares two model-based agents using handcrafted state abstractions (Tabular) or learning an internal world model (Dreamer).
- Method Summary:
  - The authors used two model-based agents, Tabular and Dreamer.
  - The Tabular agent uses handcrafted state abstractions, while the Dreamer agent learns an internal world model.
- Experiment Summary:
  - The Tabular agent was observed to show how curiosity and competence guide exploration in distinct patterns.
  - The Dreamer agent was used to reveal a two-way interaction between exploration and representation learning.
- Main Findings:
  - The study found that prioritizing both curiosity and competence improves exploration.
  - The Dreamer agent revealed a developmental co-evolution of curiosity and competence.
  - The findings formalize adaptive exploration as a balance between pursuing the unknown and the controllable.
- Limitations:
  - The paper does not explicitly mention any limitations of the study.
- Link: [http://arxiv.org/abs/2507.08210v1](http://arxiv.org/abs/2507.08210v1)

---

- Title: Optimizing Model Splitting and Device Task Assignment for Deceptive Signal Assisted Private Multi-hop Split Learning
- Authors: Dongyu Wei, Xiaoren Xu, Yuchen Liu, H. Vincent Poor, Mingzhe Chen
- Publication Date: July 9, 2025
- Key Contributions:
  - The paper investigates deceptive signal-assisted private split learning.
  - It proposes a soft actor-critic deep reinforcement learning framework with intrinsic curiosity module and cross-attention (ICM-CA) to solve the problem of determining the subset of devices used for deceptive signal transmission, model training devices, and the models assigned to each model training device.
- Method Summary:
  - The problem is formulated as an optimization problem aimed at minimizing the information leaked to eavesdroppers while meeting the model training energy consumption and delay constraints.
  - The proposed ICM-CA framework enables a centralized agent to determine the model training devices, the deceptive signal transmission devices, the transmit power, and sub-models assigned to each model training device without knowing the position and monitoring probability of eavesdroppers.
  - The ICM module encourages the server to explore novel actions and states, and a CA module determines the importance of each historical state-action pair, thus improving training efficiency.
- Experiment Summary:
  - Simulation results were used to demonstrate the effectiveness of the proposed method.
- Main Findings:
  - The proposed method improves the convergence rate by up to 3x.
  - It reduces the information leaked to eavesdroppers by up to 13% compared to the traditional SAC algorithm.
- Limitations:
  - The paper does not explicitly mention any limitations of the proposed method.
- Link: [http://arxiv.org/abs/2507.07323v1](http://arxiv.org/abs/2507.07323v1)

---

- Title: Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation Learning with Double Exploration
- Authors: Heyang Zhao, Xingrui Yu, David M. Bossens, Ivor W. Tsang, Quanquan Gu
- Publication Date: June 25, 2025
- Key Contributions: 
  - The authors propose a new imitation learning algorithm, Imitation Learning with Double Exploration (ILDE), which aims to improve the learning of expert policy from limited demonstrations.
  - The authors provide a theoretical justification of ILDE as an uncertainty-regularized policy optimization method with optimistic exploration.
- Method Summary: 
  - The ILDE algorithm implements exploration in two ways: optimistic policy optimization with an exploration bonus that rewards state-action pairs with high uncertainty, and curiosity-driven exploration of states that deviate from demonstration trajectories.
- Experiment Summary: 
  - The authors empirically test the ILDE algorithm on Atari and MuJoCo tasks, comparing its performance and sample efficiency to existing state-of-the-art imitation learning algorithms.
- Main Findings: 
  - The ILDE algorithm outperforms existing imitation learning algorithms in terms of sample efficiency.
  - It also achieves beyond-expert performance on Atari and MuJoCo tasks with fewer demonstrations than previous work.
- Limitations: 
  - The paper does not explicitly mention any limitations, but as with all research, the findings may not generalize to all environments or tasks.
- Link: [http://arxiv.org/abs/2506.20307v1](http://arxiv.org/abs/2506.20307v1)

---

- Title: Exploring Artificial Intelligence Tutor Teammate Adaptability to Harness Discovery Curiosity and Promote Learning in the Context of Interactive Molecular Dynamics
- Authors: Mustafa Demir, Jacob Miratsky, Jonathan Nguyen, Chun Kit Chan, Punya Mishra, Abhishek Singharoy
- Publication Date: June 26, 2025
- Key Contributions:
  - This study explores the impact of an AI tutor teammate on student engagement and learning effectiveness in Interactive Molecular Dynamics (IMD) tasks.
  - It investigates the role of AI's curiosity-triggering and response behaviors in stimulating student curiosity and affecting the complexity of student-initiated questions.
  - The research also assesses how AI interventions shape student engagement, foster discovery curiosity, and enhance team performance within the IMD learning environment.
- Method Summary:
  - Using a Wizard-of-Oz paradigm, a human experimenter dynamically adjusts the AI tutor teammate's behavior through a large language model.
  - A mixed-methods exploratory design was used, with 11 high school students participating in four IMD tasks that increased in complexity over a 60-minute period.
  - Team performance was evaluated through real-time observation and recordings, while team communication was measured by question complexity and AI's curiosity-triggering and response behaviors.
  - Cross Recurrence Quantification Analysis (CRQA) metrics were used to reflect structural alignment in coordination and were linked to communication behaviors.
- Experiment Summary:
  - The experiment involved 11 high school students who participated in four IMD tasks on the Visual Molecular Dynamics platform.
  - The tasks involved molecular visualization and calculations, and increased in complexity over a 60-minute period.
- Main Findings:
  - High-performing teams exhibited superior task completion, deeper understanding, and increased engagement.
  - Advanced questions were associated with AI curiosity-triggering, indicating heightened engagement and cognitive complexity.
  - CRQA metrics highlighted dynamic synchronization in student-AI interactions, emphasizing structured yet adaptive engagement to promote curiosity.
  - The AI's dual role as a teammate and educator indicates its capacity to provide adaptive feedback, sustaining engagement and epistemic curiosity.
- Limitations: The paper does not mention any specific limitations of the study.
- Link: [http://arxiv.org/abs/2506.22520v1](http://arxiv.org/abs/2506.22520v1)

---

- Title: Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning
- Authors: Duc Hieu Ho, Chenglin Fan
- Publication Date: June 19, 2025
- Key Contributions:
  - The authors conducted a comprehensive benchmark evaluation of ten widely used large language models.
  - They proposed a new prompting strategy, self-critique-guided curiosity refinement prompting, to improve the honesty and helpfulness of large language model outputs.
- Method Summary:
  - The proposed method extends the curiosity-driven prompting strategy by incorporating two lightweight in-context steps: self-critique step and refinement step.
  - This allows models to self-critique and refine their responses without additional training.
- Experiment Summary:
  - The experiment was conducted on the HONESET dataset and evaluated using the H² (honesty and helpfulness) framework.
  - GPT-4o was used as a judge of honesty and helpfulness.
- Main Findings:
  - The approach showed consistent improvements across all models, reducing the number of poor-quality responses and increasing high-quality responses.
  - It achieved relative gains in H² scores ranging from 1.4% to 4.3% compared to curiosity-driven prompting across evaluated models.
- Limitations: The paper does not mention any specific limitations.
- Link: [http://arxiv.org/abs/2506.16064v1](http://arxiv.org/abs/2506.16064v1)

---

- Title: Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning
- Authors: Kongcheng Zhang, Qi Yao, Shunyu Liu, Yingjie Wang, Baisheng Lai, Jieping Ye, Mingli Song, Dacheng Tao
- Publication Date: June 10, 2025
- Key Contributions:
  - The authors propose a novel self-rewarding reinforcement learning framework for enhancing Large Language Model (LLM) reasoning.
  - They introduce CoVo, an intrinsic reward mechanism that integrates Consistency and Volatility via a robust vector-space aggregation strategy.
- Method Summary:
  - The method leverages the consistency of intermediate reasoning states across different reasoning trajectories.
  - The CoVo mechanism is complemented by a curiosity bonus to promote diverse exploration.
  - This approach enables LLMs to perform reinforcement learning in a self-rewarding manner, offering a scalable pathway for learning to reason without external supervision.
- Experiment Summary:
  - The authors conducted extensive experiments on diverse reasoning benchmarks to test the effectiveness of their proposed method.
- Main Findings:
  - The results show that CoVo achieves performance comparable to or even surpassing supervised reinforcement learning.
- Limitations:
  - The paper does not explicitly mention any limitations of the proposed method.
- Link: [http://arxiv.org/abs/2506.08745v1](http://arxiv.org/abs/2506.08745v1)

---

- Title: A Bit of Freedom Goes a Long Way: Classical and Quantum Algorithms for Reinforcement Learning under a Generative Model
- Authors: Andris Ambainis, Joao F. Doriguello, Debbie Lim
- Publication Date: July 30, 2025
- Key Contributions:
  - The authors propose new classical and quantum online algorithms for learning finite-horizon and infinite-horizon average-reward Markov Decision Processes (MDPs).
  - They introduce a hybrid exploration-generative reinforcement learning model where the agent can interact with the environment in a generative sampling fashion.
  - They demonstrate that it is possible to avoid certain paradigms from reinforcement learning and instead compute and use optimal policies directly.
- Method Summary:
  - The authors use known classical and new quantum algorithms for approximating optimal policies under a generative model within their learning algorithms.
  - They propose a novel measure of regret for infinite-horizon MDPs.
- Experiment Summary:
  - The paper is theoretical and does not include specific experiments. Instead, it provides mathematical proofs and analysis to support the proposed algorithms.
- Main Findings:
  - For finite-horizon MDPs, the proposed quantum algorithms obtain regret bounds which only depend logarithmically on the number of time steps, breaking the classical barrier.
  - For infinite-horizon MDPs, the classical and quantum bounds maintain the dependence but with better factors related to state space size and action space size.
  - The proposed quantum algorithms have significantly better regret measures for infinite-horizon MDPs compared to classical algorithms.
- Limitations:
  - The paper does not discuss potential limitations of the proposed algorithms or their applicability to real-world problems.
- Link: [http://arxiv.org/abs/2507.22854v1](http://arxiv.org/abs/2507.22854v1)

---

- Title: RLVMR: Reinforcement Learning with Verifiable Meta-Reasoning Rewards for Robust Long-Horizon Agents
- Authors: Zijing Zhang, Ziyang Chen, Mingxiao Li, Zhaopeng Tu, Xiaolong Li
- Publication Date: July 30, 2025
- Key Contributions: 
  - The authors introduce a novel framework, RLVMR, that integrates dense, process-level supervision into end-to-end reinforcement learning (RL).
  - They address the problem of inefficient exploration in RL, which often leads to brittle agents that fail to generalize.
  - They achieve state-of-the-art results on challenging benchmarks, ALFWorld and ScienceWorld.
- Method Summary:
  - RLVMR rewards verifiable, meta-reasoning behaviors, enabling an agent to tag its cognitive steps such as planning, exploration, and reflection.
  - It provides programmatic, rule-based rewards for actions that contribute to effective problem-solving.
  - These process-centric rewards are combined with the final outcome signal and optimized using a critic-free policy gradient method.
- Experiment Summary:
  - The authors tested RLVMR on the ALFWorld and ScienceWorld benchmarks.
  - They used a 7B model for the most difficult unseen task split.
- Main Findings:
  - RLVMR achieved new state-of-the-art results, with the 7B model reaching an 83.6% success rate on the most difficult unseen task split.
  - The analysis confirmed that these gains stem from improved reasoning quality, including significant reductions in redundant actions and enhanced error recovery.
- Limitations: 
  - The paper does not explicitly mention any limitations of the RLVMR framework.
- Link: [http://arxiv.org/abs/2507.22844v1](http://arxiv.org/abs/2507.22844v1)

---

- Title: The Multi-Agent Fault Localization System Based on Monte Carlo Tree Search Approach
- Authors: Rui Ren
- Publication Date: July 30, 2025
- Key Contributions:
  - The paper introduces KnowledgeMind, a new large language model (LLM) multi-agent system for Root Cause Analysis (RCA) in microservices.
  - The system uses a Monte Carlo Tree Search and a knowledge base reward mechanism for service-by-service reasoning.
  - The proposed method significantly reduces the burden on the maximum context window length and effectively mitigates hallucinations during the inference process.
- Method Summary:
  - The KnowledgeMind system uses a Monte Carlo Tree Search approach to explore service-by-service.
  - It incorporates a rule-based real-time reward mechanism to mitigate hallucinations during the inference process.
- Experiment Summary:
  - The paper compares the proposed method with the state-of-the-art LLM for RCA methods.
  - The experiments focus on the root cause localization accuracy of the proposed method.
- Main Findings:
  - The proposed method requires only one-tenth of the maximum context window length compared to existing methods.
  - The method achieves a 49.29% to 128.35% improvement in root cause localization accuracy compared to the state-of-the-art LLM for RCA framework.
- Limitations:
  - The paper does not explicitly mention any limitations of the proposed method.
- Link: [http://arxiv.org/abs/2507.22800v1](http://arxiv.org/abs/2507.22800v1)

---

