- Title: From Curiosity to Competence: How World Models Interact with the Dynamics of Exploration
- Authors: Fryderyk Mantiuk, Hanqi Zhou, Charley M. Wu
- Publication Date: July 10, 2025
- Key Contributions: 
    - The authors bridge cognitive theories of intrinsic motivation with reinforcement learning to explore how evolving internal representations mediate the trade-off between curiosity and competence.
    - They compare two model-based agents (Tabular and Dreamer) to understand how curiosity and competence guide exploration.
- Method Summary: 
    - The study uses two model-based agents, Tabular and Dreamer, to examine the interaction between exploration and representation learning.
    - The Tabular agent uses handcrafted state abstractions, while the Dreamer agent learns an internal world model.
- Experiment Summary: 
    - The authors observed the exploration patterns of the Tabular and Dreamer agents, focusing on how curiosity and competence guide their actions.
    - They also studied the interaction between exploration and representation learning in the Dreamer agent.
- Main Findings: 
    - The Tabular agent showed that curiosity and competence guide exploration in distinct patterns and that prioritizing both improves exploration.
    - The Dreamer agent revealed a two-way interaction between exploration and representation learning, reflecting the developmental co-evolution of curiosity and competence.
- Limitations: 
    - The paper does not explicitly mention any limitations. However, the findings are based on the behavior of two specific model-based agents, which may not generalize to all types of agents.
- Link: [http://arxiv.org/abs/2507.08210v1](http://arxiv.org/abs/2507.08210v1)

---

- Title: Optimizing Model Splitting and Device Task Assignment for Deceptive Signal Assisted Private Multi-hop Split Learning
- Authors: Dongyu Wei, Xiaoren Xu, Yuchen Liu, H. Vincent Poor, Mingzhe Chen
- Publication Date: July 9, 2025
- Key Contributions:
  - The authors investigate deceptive signal-assisted private split learning.
  - They propose an optimization problem to minimize information leaked to eavesdroppers while meeting model training energy consumption and delay constraints.
  - They introduce a soft actor-critic deep reinforcement learning framework with intrinsic curiosity module and cross-attention (ICM-CA) to solve the problem.
- Method Summary:
  - The proposed method uses an ICM module to encourage the server to explore novel actions and states.
  - A CA module is used to determine the importance of each historical state-action pair, thus improving training efficiency.
  - The framework enables a centralized agent to determine the model training devices, the deceptive signal transmission devices, the transmit power, and sub-models assigned to each model training device without knowing the position and monitoring probability of eavesdroppers.
- Experiment Summary:
  - Simulation results were used to validate the proposed method.
- Main Findings:
  - The proposed method improves the convergence rate by up to 3x.
  - It reduces the information leaked to eavesdroppers by up to 13% compared to the traditional SAC algorithm.
- Limitations:
  - The paper does not mention any specific limitations of the proposed method.
- Link: [http://arxiv.org/abs/2507.07323v1](http://arxiv.org/abs/2507.07323v1)

---

- Title: Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation Learning with Double Exploration
- Authors: Heyang Zhao, Xingrui Yu, David M. Bossens, Ivor W. Tsang, Quanquan Gu
- Publication Date: June 25, 2025
- Key Contributions:
  - The authors propose a novel imitation learning algorithm, Imitation Learning with Double Exploration (ILDE).
  - They provide a theoretical justification of ILDE as an uncertainty-regularized policy optimization method with optimistic exploration.
- Method Summary:
  - The ILDE algorithm implements exploration in two aspects: optimistic policy optimization and curiosity-driven exploration.
  - Optimistic policy optimization rewards state-action pairs with high uncertainty to potentially improve the convergence to the expert policy.
  - Curiosity-driven exploration investigates states that deviate from the demonstration trajectories to potentially yield beyond-expert performance.
- Experiment Summary:
  - The authors empirically tested the ILDE algorithm on Atari and MuJoCo tasks.
  - They compared the performance of ILDE with state-of-the-art imitation learning algorithms in terms of sample efficiency.
- Main Findings:
  - The ILDE algorithm outperforms the state-of-the-art imitation learning algorithms in terms of sample efficiency.
  - ILDE achieves beyond-expert performance on Atari and MuJoCo tasks with fewer demonstrations than in previous work.
- Limitations:
  - The paper does not explicitly mention any limitations of the ILDE algorithm.
- Link: [http://arxiv.org/abs/2506.20307v1](http://arxiv.org/abs/2506.20307v1)

---

- Title: A Bit of Freedom Goes a Long Way: Classical and Quantum Algorithms for Reinforcement Learning under a Generative Model
- Authors: Andris Ambainis, Joao F. Doriguello, Debbie Lim
- Publication Date: July 30, 2025
- Key Contributions:
  - The authors propose new classical and quantum online algorithms for learning finite-horizon and infinite-horizon average-reward Markov Decision Processes (MDPs).
  - They introduce a hybrid exploration-generative reinforcement learning model where the agent can interact with the environment in a generative sampling fashion.
  - The authors demonstrate that it is possible to avoid certain paradigms from reinforcement learning and instead directly compute and use optimal policies, resulting in better regret bounds.
- Method Summary:
  - The authors employ known classical and new quantum algorithms for approximating optimal policies under a generative model within their learning algorithms.
  - They propose a novel measure of regret for infinite-horizon MDPs.
- Experiment Summary:
  - The paper is theoretical and does not include specific experiments.
- Main Findings:
  - For finite-horizon MDPs, the proposed quantum algorithms obtain regret bounds that depend logarithmically on the number of time steps, breaking the classical barrier.
  - For infinite-horizon MDPs, the classical and quantum bounds maintain the dependence but with better factors related to state space size and action space size.
  - The quantum algorithms have exponentially better regret compared to classical algorithms when measured with the novel measure of regret for infinite-horizon MDPs.
- Limitations:
  - The paper does not discuss any limitations of the proposed methods.
- Link: [http://arxiv.org/abs/2507.22854v1](http://arxiv.org/abs/2507.22854v1)

---

- Title: RLVMR: Reinforcement Learning with Verifiable Meta-Reasoning Rewards for Robust Long-Horizon Agents
- Authors: Zijing Zhang, Ziyang Chen, Mingxiao Li, Zhaopeng Tu, Xiaolong Li
- Publication Date: July 30, 2025
- Key Contributions:
  - The authors introduce RLVMR, a new framework that integrates process-level supervision into end-to-end reinforcement learning (RL).
  - They propose the use of verifiable, meta-reasoning behaviors as rewards to address the problem of inefficient exploration in RL.
  - The authors demonstrate that their approach leads to more robust, efficient, and interpretable agents.
- Method Summary:
  - RLVMR allows an agent to tag its cognitive steps, such as planning, exploration, and reflection.
  - It provides rule-based rewards for actions that contribute to effective problem-solving.
  - These process-centric rewards are combined with the final outcome signal and optimized using a critic-free policy gradient method.
- Experiment Summary:
  - The authors tested RLVMR on the challenging ALFWorld and ScienceWorld benchmarks.
- Main Findings:
  - RLVMR achieved new state-of-the-art results, with their 7B model reaching an 83.6% success rate on the most difficult unseen task split.
  - The gains were confirmed to stem from improved reasoning quality, including significant reductions in redundant actions and enhanced error recovery.
- Limitations:
  - The paper does not explicitly mention any limitations of the study.
- Link: [http://arxiv.org/abs/2507.22844v1](http://arxiv.org/abs/2507.22844v1)

---

- Title: The Multi-Agent Fault Localization System Based on Monte Carlo Tree Search Approach
- Authors: Rui Ren
- Publication Date: July 30, 2025
- Key Contributions:
  - The paper introduces KnowledgeMind, a novel multi-agent system for Root Cause Analysis (RCA) that uses a Monte Carlo Tree Search approach and a knowledge base reward mechanism.
  - The proposed system significantly reduces the burden on the maximum context window length, requiring only one-tenth of its size.
  - The system incorporates a rule-based real-time reward mechanism to effectively mitigate hallucinations during the inference process.
- Method Summary:
  - The method involves using a large language model (LLM) for RCA, leveraging its generalization ability combined with expert experience.
  - The system uses a Monte Carlo Tree Search approach for service-by-service reasoning.
  - A knowledge base reward mechanism is used to standardize the reasoning process.
- Experiment Summary:
  - The paper compares the proposed method with State-Of-The-Art (SOTA) LLM for RCA methods.
- Main Findings:
  - The proposed method achieves a 49.29% to 128.35% improvement in root cause localization accuracy compared to the SOTA LLM for RCA framework.
- Limitations:
  - The paper does not mention any specific limitations of the proposed method.
- Link: [The Multi-Agent Fault Localization System Based on Monte Carlo Tree Search Approach](http://arxiv.org/abs/2507.22800v1)

---

- Title: Deep reinforcement learning for efficient exploration of combinatorial structural design spaces
- Authors: Chloe S. H. Hong, Keith J. Lee, Caitlin T. Mueller
- Publication Date: July 30, 2025
- Key Contributions:
  - Proposes a reinforcement learning framework for performance-driven structural design.
  - Models structures as compositions of predefined elements, aligning form finding with practical constraints.
  - Adapts reinforcement learning for structural design, formulating the design task as a sequential decision-making problem.
- Method Summary:
  - The method combines bottom-up design generation with learned strategies to efficiently search large combinatorial design spaces.
  - It models structures as compositions of predefined elements, aligning form finding with practical constraints like constructability and component reuse.
  - The design task is formulated as a sequential decision-making problem, and a human learning-inspired training algorithm is used.
- Experiment Summary:
  - The framework is demonstrated by designing steel braced truss frame cantilever structures.
  - Trained policies consistently generate distinct, high-performing designs that display structural performance and material efficiency.
- Main Findings:
  - The method generates high-performing designs that align with known engineering principles.
  - The agent efficiently narrows its search to promising regions of the design space, revealing transferable structural knowledge.
- Limitations:
  - The paper does not explicitly mention any limitations of the proposed method.
- Link: [http://arxiv.org/abs/2507.22804v1](http://arxiv.org/abs/2507.22804v1)

---

