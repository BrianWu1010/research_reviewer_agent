- Title: From Curiosity to Competence: How World Models Interact with the Dynamics of Exploration
- Authors: Fryderyk Mantiuk, Hanqi Zhou, Charley M. Wu
- Publication Date: July 10, 2025
- Key Contributions:
  - The paper bridges cognitive theories of intrinsic motivation with reinforcement learning.
  - The authors explore how evolving internal representations mediate the trade-off between curiosity and competence.
  - The research compares two model-based agents: one using handcrafted state abstractions (Tabular) and the other learning an internal world model (Dreamer).
- Method Summary:
  - The authors use two different agents for their study: Tabular and Dreamer.
  - Tabular uses handcrafted state abstractions, while Dreamer learns an internal world model.
- Experiment Summary:
  - The Tabular agent was observed for patterns in how curiosity and competence guide exploration.
  - Both curiosity and competence were prioritized to see how this improves exploration.
  - The Dreamer agent was used to observe the interaction between exploration and representation learning.
- Main Findings:
  - The Tabular agent shows that curiosity and competence guide exploration in distinct patterns.
  - Prioritizing both curiosity and competence improves exploration.
  - The Dreamer agent reveals a two-way interaction between exploration and representation learning.
  - The findings suggest that adaptive exploration is a balance between pursuing the unknown and the controllable.
- Limitations:
  - The paper does not explicitly mention any limitations of the study.
- Link: [http://arxiv.org/abs/2507.08210v1](http://arxiv.org/abs/2507.08210v1)

---

- Title: Optimizing Model Splitting and Device Task Assignment for Deceptive Signal Assisted Private Multi-hop Split Learning
- Authors: Dongyu Wei, Xiaoren Xu, Yuchen Liu, H. Vincent Poor, Mingzhe Chen
- Publication Date: July 9, 2025
- Key Contributions:
  - The paper investigates deceptive signal-assisted private split learning.
  - Proposes a soft actor-critic deep reinforcement learning framework with intrinsic curiosity module and cross-attention (ICM-CA) to solve the optimization problem.
- Method Summary:
  - The problem of determining the subset of devices for deceptive signal transmission, the subset of model training devices, and the models assigned to each model training device is formulated as an optimization problem.
  - The goal is to minimize the information leaked to eavesdroppers while meeting the model training energy consumption and delay constraints.
  - A soft actor-critic deep reinforcement learning framework with ICM-CA is proposed to solve this problem.
  - The ICM module encourages the server to explore novel actions and states and a CA module determines the importance of each historical state-action pair, improving training efficiency.
- Experiment Summary:
  - Simulation results were used to demonstrate the effectiveness of the proposed method.
- Main Findings:
  - The proposed method improves the convergence rate by up to 3x.
  - It reduces the information leaked to eavesdroppers by up to 13% compared to the traditional SAC algorithm.
- Limitations:
  - The paper does not mention any specific limitations of the proposed method.
- Link: [http://arxiv.org/abs/2507.07323v1](http://arxiv.org/abs/2507.07323v1)

---

- Title: Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation Learning with Double Exploration
- Authors: Heyang Zhao, Xingrui Yu, David M. Bossens, Ivor W. Tsang, Quanquan Gu
- Publication Date: June 25, 2025
- Key Contributions:
  - The authors propose a novel imitation learning algorithm called Imitation Learning with Double Exploration (ILDE).
  - They provide a theoretical justification of ILDE as an uncertainty-regularized policy optimization method with optimistic exploration.
- Method Summary:
  - The ILDE algorithm implements exploration in two aspects: optimistic policy optimization via an exploration bonus and curiosity-driven exploration of the states that deviate from the demonstration trajectories.
  - The exploration bonus rewards state-action pairs with high uncertainty to potentially improve the convergence to the expert policy.
- Experiment Summary:
  - The authors test the ILDE algorithm on Atari and MuJoCo tasks, comparing its performance with state-of-the-art imitation learning algorithms.
- Main Findings:
  - The ILDE algorithm outperforms the state-of-the-art imitation learning algorithms in terms of sample efficiency.
  - ILDE achieves beyond-expert performance on Atari and MuJoCo tasks with fewer demonstrations than in previous work.
- Limitations:
  - The paper does not explicitly mention any limitations of the ILDE algorithm.
- Link: [http://arxiv.org/abs/2506.20307v1](http://arxiv.org/abs/2506.20307v1)

---

- Title: A Bit of Freedom Goes a Long Way: Classical and Quantum Algorithms for Reinforcement Learning under a Generative Model
- Authors: Andris Ambainis, Joao F. Doriguello, Debbie Lim
- Publication Date: July 30, 2025
- Key Contributions:
  - Propose novel classical and quantum online algorithms for learning finite-horizon and infinite-horizon average-reward Markov Decision Processes (MDPs).
  - Introduce a hybrid exploration-generative reinforcement learning model where the agent can interact with the environment in a generative sampling fashion.
  - Show that it's possible to avoid several paradigms from reinforcement learning and instead compute and use optimal policies directly.
  - Break the classical barrier of regret bounds for finite-horizon MDPs with quantum algorithms.
  - Propose a novel measure of regret for infinite-horizon MDPs.
- Method Summary:
  - The authors use known classical and new quantum algorithms for approximating optimal policies under a generative model within their learning algorithms.
  - They avoid traditional reinforcement learning paradigms and instead compute and use optimal policies directly.
- Experiment Summary:
  - The paper is theoretical and does not include specific experiments.
- Main Findings:
  - The proposed quantum algorithms obtain regret bounds which only depend logarithmically on the number of time steps, breaking the classical barrier.
  - For infinite-horizon MDPs, the classical and quantum bounds still maintain the dependence but with better factors related to state space size and action space size.
  - The proposed quantum algorithms have a significantly better measure of regret for infinite-horizon MDPs compared to classical algorithms.
- Limitations:
  - The paper does not mention any specific limitations.
- Link: [http://arxiv.org/abs/2507.22854v1](http://arxiv.org/abs/2507.22854v1)

---

- Title: RLVMR: Reinforcement Learning with Verifiable Meta-Reasoning Rewards for Robust Long-Horizon Agents
- Authors: Zijing Zhang, Ziyang Chen, Mingxiao Li, Zhaopeng Tu, Xiaolong Li
- Publication Date: July 30, 2025
- Key Contributions:
  - Introduction of RLVMR, a novel framework that integrates dense, process-level supervision into end-to-end reinforcement learning (RL).
  - The framework rewards verifiable, meta-reasoning behaviors, improving the reasoning quality of autonomous agents.
  - Achieved new state-of-the-art results on the ALFWorld and ScienceWorld benchmarks.
- Method Summary:
  - RLVMR equips an agent to explicitly tag its cognitive steps, such as planning, exploration, and reflection.
  - It provides programmatic, rule-based rewards for actions that contribute to effective problem-solving.
  - These process-centric rewards are combined with the final outcome signal and optimized using a critic-free policy gradient method.
- Experiment Summary:
  - The RLVMR model was tested on the challenging ALFWorld and ScienceWorld benchmarks.
  - The performance of the model was evaluated based on its success rate on unseen task splits.
- Main Findings:
  - The 7B model of RLVMR achieved an 83.6% success rate on the most difficult unseen task split.
  - The gains stem from improved reasoning quality, including significant reductions in redundant actions and enhanced error recovery.
  - This led to more robust, efficient, and interpretable agents.
- Limitations:
  - The paper does not explicitly mention any limitations of the study. However, it is implied that traditional RL methods that optimize solely for final task success often reinforce flawed or inefficient reasoning paths.
- Link: [RLVMR: Reinforcement Learning with Verifiable Meta-Reasoning Rewards for Robust Long-Horizon Agents](http://arxiv.org/abs/2507.22844v1)

---

- Title: Deep reinforcement learning for efficient exploration of combinatorial structural design spaces
- Authors: Chloe S. H. Hong, Keith J. Lee, Caitlin T. Mueller
- Publication Date: July 30, 2025
- Key Contributions:
  - Proposes a reinforcement learning framework for performance-driven structural design.
  - Combines bottom-up design generation with learned strategies to efficiently search large combinatorial design spaces.
  - Models structures as compositions of predefined elements, aligning form finding with practical constraints like constructability and component reuse.
- Method Summary:
  - The design task is formulated as a sequential decision-making problem.
  - A human learning inspired training algorithm is used.
  - Reinforcement learning is adapted for structural design.
- Experiment Summary:
  - The framework is demonstrated by designing steel braced truss frame cantilever structures.
  - Trained policies consistently generate distinct, high-performing designs.
- Main Findings:
  - The designs display structural performance and material efficiency.
  - The use of structural strategies aligns with known engineering principles.
  - The agent efficiently narrows its search to promising regions of the design space, revealing transferable structural knowledge.
- Limitations: The paper does not explicitly mention any limitations.
- Link: [http://arxiv.org/abs/2507.22804v1](http://arxiv.org/abs/2507.22804v1)

---

- Title: The Multi-Agent Fault Localization System Based on Monte Carlo Tree Search Approach
- Authors: Rui Ren
- Publication Date: July 30, 2025
- Key Contributions:
  - The paper introduces KnowledgeMind, an innovative Large Language Model (LLM) multi-agent system for Root Cause Analysis (RCA).
  - The proposed system uses a Monte Carlo Tree Search and a knowledge base reward mechanism for standardized service-by-service reasoning.
  - The system significantly reduces the burden on the maximum context window length, requiring only one-tenth of its size.
  - The system incorporates a rule-based real-time reward mechanism to effectively mitigate hallucinations during the inference process.
- Method Summary:
  - The method involves using a Monte Carlo Tree Search approach to explore service-by-service.
  - The system uses a knowledge base reward mechanism to standardize reasoning.
  - The system uses a rule-based real-time reward mechanism to mitigate hallucinations during the inference process.
- Experiment Summary:
  - The paper does not provide specific details about the experiments conducted to test the proposed system.
- Main Findings:
  - The proposed method achieves a 49.29% to 128.35% improvement in root cause localization accuracy compared to the State-Of-The-Art (SOTA) LLM for RCA methods.
- Limitations:
  - The paper does not provide specific details about the limitations of the proposed system.
- Link: [http://arxiv.org/abs/2507.22800v1](http://arxiv.org/abs/2507.22800v1)

---

