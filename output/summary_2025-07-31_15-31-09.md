- Title: From Curiosity to Competence: How World Models Interact with the Dynamics of Exploration
- Authors: Fryderyk Mantiuk, Hanqi Zhou, Charley M. Wu
- Publication Date: July 10, 2025
- Key Contributions:
  - The study bridges cognitive theories of intrinsic motivation with reinforcement learning.
  - It explores how evolving internal representations mediate the trade-off between curiosity and competence.
  - It introduces a comparison between two model-based agents using handcrafted state abstractions (Tabular) or learning an internal world model (Dreamer).
- Method Summary:
  - The authors used two model-based agents: Tabular and Dreamer.
  - Tabular uses handcrafted state abstractions, while Dreamer learns an internal world model.
  - The study examines how these agents balance curiosity (novelty or information gain) and competence (empowerment).
- Experiment Summary:
  - The Tabular agent's exploration patterns were studied to understand how curiosity and competence guide exploration.
  - The Dreamer agent was used to examine the interaction between exploration and representation learning.
- Main Findings:
  - The Tabular agent shows that curiosity and competence guide exploration in distinct patterns, and prioritizing both improves exploration.
  - The Dreamer agent reveals a two-way interaction between exploration and representation learning, mirroring the developmental co-evolution of curiosity and competence.
- Limitations:
  - The paper does not explicitly mention any limitations of the study.
- Link: [http://arxiv.org/abs/2507.08210v1](http://arxiv.org/abs/2507.08210v1)

---

- Title: Optimizing Model Splitting and Device Task Assignment for Deceptive Signal Assisted Private Multi-hop Split Learning
- Authors: Dongyu Wei, Xiaoren Xu, Yuchen Liu, H. Vincent Poor, Mingzhe Chen
- Publication Date: July 9, 2025
- Key Contributions:
  - The paper investigates deceptive signal-assisted private split learning.
  - It formulates an optimization problem to minimize the information leaked to eavesdroppers while meeting the model training energy consumption and delay constraints.
  - A soft actor-critic deep reinforcement learning framework with intrinsic curiosity module and cross-attention (ICM-CA) is proposed to solve this problem.
- Method Summary:
  - The proposed method uses an ICM module to encourage the server to explore novel actions and states.
  - A CA module is used to determine the importance of each historical state-action pair, thus improving training efficiency.
  - The method enables a centralized agent to determine the model training devices, the deceptive signal transmission devices, the transmit power, and sub-models assigned to each model training device without knowing the position and monitoring probability of eavesdroppers.
- Experiment Summary:
  - Simulation results were used to evaluate the proposed method.
- Main Findings:
  - The proposed method improves the convergence rate by up to 3x.
  - It reduces the information leaked to eavesdroppers by up to 13% compared to the traditional SAC algorithm.
- Limitations: 
  - The paper does not mention any specific limitations of the proposed method.
- Link: [http://arxiv.org/abs/2507.07323v1](http://arxiv.org/abs/2507.07323v1)

---

- Title: Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation Learning with Double Exploration
- Authors: Heyang Zhao, Xingrui Yu, David M. Bossens, Ivor W. Tsang, Quanquan Gu
- Publication Date: June 25, 2025
- Key Contributions:
  - The authors propose a novel imitation learning algorithm, Imitation Learning with Double Exploration (ILDE), to overcome challenges in learning expert policy from limited demonstrations.
  - They provide a theoretical justification of ILDE as an uncertainty-regularized policy optimization method with optimistic exploration.
- Method Summary:
  - ILDE implements exploration in two aspects: optimistic policy optimization via an exploration bonus that rewards state-action pairs with high uncertainty, and curiosity-driven exploration of the states that deviate from the demonstration trajectories.
  - This approach aims to improve the convergence to the expert policy and potentially yield beyond-expert performance.
- Experiment Summary:
  - The authors empirically tested ILDE against state-of-the-art imitation learning algorithms.
  - The experiments were conducted on Atari and MuJoCo tasks with fewer demonstrations than in previous work.
- Main Findings:
  - ILDE outperforms the state-of-the-art imitation learning algorithms in terms of sample efficiency.
  - It achieves beyond-expert performance on Atari and MuJoCo tasks with fewer demonstrations.
- Limitations:
  - The paper does not explicitly mention any limitations of the proposed method. However, like any other machine learning algorithm, potential limitations could include dependency on the quality of demonstrations and the complexity of the task.
- Link: [http://arxiv.org/abs/2506.20307v1](http://arxiv.org/abs/2506.20307v1)

---

- Title: On the origins of oxygen: ALMA and JWST characterise the multi-phase, metal-enriched, star-bursting medium within a 'normal' $z > 11$ galaxy
- Authors: Joris Witstok, Renske Smit, William M. Baker, Pierluigi Rinaldi, Kevin N. Hainline, Hiddo S. B. Algera, Santiago Arribas, Tom J. L. C. Bakx, Andrew J. Bunker, Stefano Carniani, Stéphane Charlot, Jacopo Chevallard, Mirko Curti, Emma Curtis-Lake, Daniel J. Eisenstein, Kasper E. Heintz, Jakob M. Helton, Gareth C. Jones, Roberto Maiolino, Michael V. Maseda, Pablo G. Pérez-González, Clara L. Pollock, Brant E. Robertson, Aayush Saxena, Jan Scholtz, Irene Shivaei, Fengwu Sun, Sandro Tacchella, Hannah Übler, Darach Watson, Chris J. Willott, Zihao Wu
- Publication Date: 30th July 2025
- Key Contributions: 
  - The paper presents new observations of a typical galaxy at $z > 11$ using ALMA.
  - It confirms the presence of the [O III] 88 $\\mu$m line and refines the redshift measurement.
  - The study provides insights into the nature of early galaxies and their formation mechanisms.
- Method Summary: 
  - The authors used deep ALMA observations to study the galaxy JADES-GS-z11-0.
  - They confirmed the presence of the [O III] 88 $\\mu$m line and refined the redshift measurement.
- Experiment Summary: 
  - The study involved observing the galaxy JADES-GS-z11-0 using ALMA.
  - The authors confirmed the presence of the [O III] 88 $\\mu$m line and refined the redshift measurement.
- Main Findings: 
  - The study confirmed the presence of the [O III] 88 $\\mu$m line in the galaxy JADES-GS-z11-0.
  - The redshift measurement was refined, which helps to avoid systematic overestimates.
  - The study suggests that the galaxy consists of two low-mass components undergoing strong bursts of star formation, pre-enriched in oxygen, only 400 Myr after the Big Bang.
- Limitations: 
  - The underlying dust continuum in the galaxy remains undetected.
  - The study relies on the assumption that redshifts measured purely from the Lyman-$\\alpha$ break should properly take into account the effects of potential damped Lyman-$\\alpha$ absorption systems.
- Link: [http://arxiv.org/abs/2507.22888v1](http://arxiv.org/abs/2507.22888v1)

---

- Title: Where to show Demos in Your Prompt: A Positional Bias of In-Context Learning
- Authors: Kwesi Cobbina, Tianyi Zhou
- Publication Date: July 30, 2025
- Key Contributions:
  - The paper investigates a new positional bias of In-Context Learning (ICL), referred to as Demos' Position in Prompt (DPP) bias.
  - It introduces two metrics, Accuracy-Change and Prediction-Change, to quantify net gains and output volatility induced by changes in the demos' position.
- Method Summary:
  - The authors designed a systematic evaluation pipeline to study this type of positional bias across classification, question answering, summarization, and reasoning tasks.
- Experiment Summary:
  - Extensive experiments were conducted on ten large language models (LLMs) from four open-source model families (QWEN, LLAMA3, MISTRAL, COHERE).
- Main Findings:
  - The positional bias significantly affects the accuracy and predictions of LLMs.
  - Placing demos at the start of the prompt yields the most stable and accurate outputs with gains of up to +6 points.
  - Placing demos at the end of the user message flips over 30% of predictions without improving correctness on QA tasks.
  - Smaller models are most affected by this sensitivity, though even large models remain marginally affected on complex tasks.
- Limitations:
  - The paper does not explicitly mention any limitations. However, it suggests that even large models remain marginally affected by the positional bias on complex tasks, indicating a potential area for further research.
- Link: [http://arxiv.org/abs/2507.22887v1](http://arxiv.org/abs/2507.22887v1)

---

- Title: Viser: Imperative, Web-based 3D Visualization in Python
- Authors: Brent Yi, Chung Min Kim, Justin Kerr, Gina Wu, Rebecca Feng, Anthony Zhang, Jonas Kulhanek, Hongsuk Choi, Yi Ma, Matthew Tancik, Angjoo Kanazawa
- Publication Date: July 30, 2025
- Key Contributions: 
    - Development of Viser, a 3D visualization library for computer vision and robotics.
    - Provision of a comprehensive set of 3D scene and 2D GUI primitives.
    - Creation of an imperative-style API and a web-based viewer to improve compatibility with modern programming patterns and workflows.
- Method Summary: 
    - The authors developed Viser using Python.
    - They provided a comprehensive set of 3D scene and 2D GUI primitives.
    - They used an imperative-style API and a web-based viewer to enhance compatibility with modern programming patterns and workflows.
- Experiment Summary: 
    - The paper does not provide specific details about any experiments conducted. It is more of a technical report describing Viser's features, interface, and implementation.
- Main Findings: 
    - Viser brings easy and extensible 3D visualization to Python.
    - Its features can be used independently with minimal setup or composed to build specialized interfaces.
- Limitations: 
    - The paper does not mention any limitations. However, as it is a technical report, the effectiveness of Viser may need to be evaluated in practical applications.
- Link: [Viser: Imperative, Web-based 3D Visualization in Python](http://arxiv.org/abs/2507.22885v1)

---

- Title: Deep reinforcement learning for efficient exploration of combinatorial structural design spaces
- Authors: Chloe S. H. Hong, Keith J. Lee, Caitlin T. Mueller
- Publication Date: July 30, 2025
- Key Contributions:
  - Proposes a reinforcement learning framework for performance-driven structural design.
  - Combines bottom-up design generation with learned strategies to efficiently search large combinatorial design spaces.
  - Models structures as compositions of predefined elements, aligning form finding with practical constraints.
- Method Summary:
  - The method models the design task as a sequential decision-making problem.
  - It uses a human learning-inspired training algorithm to adapt reinforcement learning for structural design.
- Experiment Summary:
  - The framework is demonstrated by designing steel braced truss frame cantilever structures.
  - Trained policies consistently generate distinct, high-performing designs.
- Main Findings:
  - The designs display structural performance and material efficiency.
  - The use of structural strategies aligns with known engineering principles.
  - The agent efficiently narrows its search to promising regions of the design space, revealing transferable structural knowledge.
- Limitations:
  - The paper does not explicitly mention any limitations of the proposed method.
- Link: [http://arxiv.org/abs/2507.22804v1](http://arxiv.org/abs/2507.22804v1)

---

- Title: HOLA: Enhancing Audio-visual Deepfake Detection via Hierarchical Contextual Aggregations and Efficient Pre-training
- Authors: Xuecheng Wu, Danlei Huang, Heli Sun, Xinyi Yin, Yifan Wang, Hao Wang, Jia Zhang, Fei Wang, Peihao Guo, Suyu Xing, Junxiao Xue, Liang He
- Publication Date: July 30, 2025
- Key Contributions:
  - The authors introduced HOLA, a solution for video-level deepfake detection.
  - They scaled audio-visual self-supervised pre-training in the multimodal video-level deepfake detection using a self-built dataset of 1.81M samples.
  - They proposed a pseudo supervised signal injection strategy to further enhance the model's performance.
- Method Summary:
  - HOLA uses an iterative-aware cross-modal learning module for selective audio-visual interactions.
  - It employs hierarchical contextual modeling with gated aggregations under the local-global perspective.
  - A pyramid-like refiner is used for scale-aware cross-grained semantic enhancements.
- Experiment Summary:
  - The authors conducted extensive experiments across expert models and MLLMs to demonstrate the effectiveness of HOLA.
  - They also conducted a series of ablation studies to explore the crucial design factors of their introduced components.
- Main Findings:
  - The HOLA model ranked 1st, outperforming the second by 0.0476 AUC on the TestA set, demonstrating its effectiveness in detecting deepfakes.
- Limitations: The paper does not explicitly mention any limitations of the study.
- Link: [http://arxiv.org/abs/2507.22781v1](http://arxiv.org/abs/2507.22781v1)

---

- Title: Resource-Efficient Adaptation of Large Language Models for Text Embeddings via Prompt Engineering and Contrastive Fine-tuning
- Authors: Benedikt Roth, Stephan Rappensperger, Tianming Qiu, Hamza Imamović, Julian Wörmann, Hao Shen
- Publication Date: July 30, 2025
- Key Contributions:
  - The authors propose an efficient method to adapt Large Language Models (LLMs) for text embeddings.
  - They introduce a combination of various aggregation techniques, task-specific prompt engineering, and text-level augmentation via contrastive fine-tuning.
- Method Summary:
  - The authors explore several adaptation strategies for pre-trained, decoder-only LLMs.
  - These strategies include various aggregation techniques for token embeddings, task-specific prompt engineering, and text-level augmentation via contrastive fine-tuning.
  - The combination of these components results in improved performance.
- Experiment Summary:
  - The proposed method was tested on the English clustering track of the Massive Text Embedding Benchmark (MTEB).
  - The attention map was analyzed to understand the shift in focus from prompt tokens to semantically relevant words.
- Main Findings:
  - The proposed method yields state-of-the-art performance on the MTEB.
  - Fine-tuning shifts focus from prompt tokens to semantically relevant words, indicating more effective compression of meaning into the final hidden state.
  - LLMs can be effectively adapted as text embedding models through a combination of prompt engineering and resource-efficient contrastive fine-tuning on synthetically generated positive pairs.
- Limitations: The paper does not mention any specific limitations of the proposed method.
- Link: [http://arxiv.org/abs/2507.22729v1](http://arxiv.org/abs/2507.22729v1)

---

